{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6db2cb01-db11-4d18-9f03-d8739feb0adf",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "Measurements of model inference time performed on a laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3689cda8-054a-44fc-8415-63efd9a9cbaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from product_classificator import Classificator\n",
    "from product_classificator.utils import SpeedTest\n",
    "from product_classificator.training.utils import get_images_from_zip\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "525a3333-e829-4094-93f7-f5be8406bee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_parquet('D:/HorizontalML/test_short.parquet')\n",
    "\n",
    "image_names = test.nm.apply(lambda x: str(x) + '.jpg').values\n",
    "images = get_images_from_zip(image_names, 'D:/HorizontalML/wb_school_horizcv_images.zip')\n",
    "texts = test.description.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a884b88a-87d4-424c-9a09-8cdf62ed6880",
   "metadata": {},
   "source": [
    "# Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec09f26b-4e92-41e8-8b38-35cbb0dd69c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "clf = Classificator(device=device)\n",
    "\n",
    "st = SpeedTest(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce289469-ef13-4353-bde3-4eb78d8263a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warming up: 100%|████████████████████████████████████████████████████████████████████| 100/100 [00:05<00:00, 18.86it/s]\n",
      "Inference (batch: 1): 100%|████████████████████████████████████████████████████████| 1255/1255 [01:07<00:00, 18.68it/s]\n",
      "Inference (batch: 32): 100%|███████████████████████████████████████████████████████████| 39/39 [00:50<00:00,  1.30s/it]\n",
      "Inference (batch: 64): 100%|███████████████████████████████████████████████████████████| 19/19 [00:49<00:00,  2.58s/it]\n"
     ]
    }
   ],
   "source": [
    "test_results = pd.DataFrame(\n",
    "    columns = pd.MultiIndex.from_product([['CUDA', 'CPU'], ['sample', 'batch64']]),\n",
    ")\n",
    "\n",
    "st.test_inference(texts, images, batch_size=1, warm_iterations=100)\n",
    "test_results.loc['base', ('CUDA', 'sample')] = np.mean(st.tests[-1]['log'])\n",
    "\n",
    "for size, col in zip([32, 64], ['batch32', 'batch64']):\n",
    "    st.test_inference(texts, images, batch_size=size)\n",
    "    test_results.loc['base', ('CUDA', col)] = np.mean(st.tests[-1]['log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "553e6227-4afa-4c1e-87e4-2ad83895a7a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference (batch: 1): 100%|██████████████████████████████████████████████████████████| 640/640 [08:16<00:00,  1.29it/s]\n",
      "Inference (batch: 32): 100%|███████████████████████████████████████████████████████████| 20/20 [07:30<00:00, 22.51s/it]\n",
      "Inference (batch: 64): 100%|███████████████████████████████████████████████████████████| 10/10 [07:03<00:00, 42.40s/it]\n"
     ]
    }
   ],
   "source": [
    "clf = clf.to('cpu')\n",
    "\n",
    "for size, col in zip([1, 32, 64], ['sample', 'batch32', 'batch64']):\n",
    "    st.test_inference(texts[:640], images[:640], batch_size=size)\n",
    "    test_results.loc['base', ('CPU', col)] = np.mean(st.tests[-1]['log'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbebf98-b6dc-42b5-bd0a-81be175cfaa2",
   "metadata": {},
   "source": [
    "# Model converted to ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f66364c-b9c5-4934-abb2-6af9e9ed6eff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf.to_onnx_clip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "848934a8-914b-40eb-91f3-640e109064b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference (batch: 1): 100%|██████████████████████████████████████████████████████████| 640/640 [05:17<00:00,  2.02it/s]\n",
      "Inference (batch: 32): 100%|███████████████████████████████████████████████████████████| 20/20 [04:22<00:00, 13.11s/it]\n",
      "Inference (batch: 64): 100%|███████████████████████████████████████████████████████████| 10/10 [04:19<00:00, 25.95s/it]\n"
     ]
    }
   ],
   "source": [
    "for size, col in zip([1, 32, 64], ['sample', 'batch32', 'batch64']):\n",
    "    st.test_inference(texts[:640], images[:640], batch_size=size)\n",
    "    test_results.loc['onnx', ('CPU', col)] = np.mean(st.tests[-1]['log'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be9f5ae1-c01a-46a1-8e94-859cdc37edcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference (batch: 1): 100%|████████████████████████████████████████████████████████| 1255/1255 [01:02<00:00, 19.95it/s]\n",
      "Inference (batch: 32): 100%|███████████████████████████████████████████████████████████| 39/39 [00:52<00:00,  1.34s/it]\n",
      "Inference (batch: 64): 100%|███████████████████████████████████████████████████████████| 19/19 [14:17<00:00, 45.12s/it]\n"
     ]
    }
   ],
   "source": [
    "clf.to('cuda')\n",
    "\n",
    "for size, col in zip([1, 32, 64], ['sample', 'batch32', 'batch64']):\n",
    "    st.test_inference(texts, images, batch_size=size)\n",
    "    test_results.loc['onnx', ('CUDA', col)] = np.mean(st.tests[-1]['log'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136d2c5b-c898-4688-8efe-163c1d31daba",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ad0ca69-075f-401b-a1c6-be96d333d940",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'OS': 'Windows',\n",
       " 'CPU': 'Intel64 Family 6 Model 158 Stepping 13, GenuineIntel',\n",
       " 'CPU cores': 12,\n",
       " 'GPUs': {0: 'GPU 0: NVIDIA GeForce RTX 2060 (6 GB)'},\n",
       " 'GPU driver': '555.85',\n",
       " 'RAM': '32 GB'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st.machine_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "625cad1d-a671-4584-b5a3-17f80f2aff5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_4f0c0_row0_col3, #T_4f0c0_row0_col4, #T_4f0c0_row1_col0, #T_4f0c0_row1_col1, #T_4f0c0_row1_col2, #T_4f0c0_row1_col5 {\n",
       "  background-color: lightgreen;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_4f0c0\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_4f0c0_level0_col0\" class=\"col_heading level0 col0\" colspan=\"3\">CPU</th>\n",
       "      <th id=\"T_4f0c0_level0_col3\" class=\"col_heading level0 col3\" colspan=\"3\">CUDA</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"blank level1\" >&nbsp;</th>\n",
       "      <th id=\"T_4f0c0_level1_col0\" class=\"col_heading level1 col0\" >batch32</th>\n",
       "      <th id=\"T_4f0c0_level1_col1\" class=\"col_heading level1 col1\" >batch64</th>\n",
       "      <th id=\"T_4f0c0_level1_col2\" class=\"col_heading level1 col2\" >sample</th>\n",
       "      <th id=\"T_4f0c0_level1_col3\" class=\"col_heading level1 col3\" >batch32</th>\n",
       "      <th id=\"T_4f0c0_level1_col4\" class=\"col_heading level1 col4\" >batch64</th>\n",
       "      <th id=\"T_4f0c0_level1_col5\" class=\"col_heading level1 col5\" >sample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4f0c0_level0_row0\" class=\"row_heading level0 row0\" >base</th>\n",
       "      <td id=\"T_4f0c0_row0_col0\" class=\"data row0 col0\" >22.506521</td>\n",
       "      <td id=\"T_4f0c0_row0_col1\" class=\"data row0 col1\" >42.394950</td>\n",
       "      <td id=\"T_4f0c0_row0_col2\" class=\"data row0 col2\" >0.775298</td>\n",
       "      <td id=\"T_4f0c0_row0_col3\" class=\"data row0 col3\" >1.297878</td>\n",
       "      <td id=\"T_4f0c0_row0_col4\" class=\"data row0 col4\" >2.579755</td>\n",
       "      <td id=\"T_4f0c0_row0_col5\" class=\"data row0 col5\" >0.052864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4f0c0_level0_row1\" class=\"row_heading level0 row1\" >onnx</th>\n",
       "      <td id=\"T_4f0c0_row1_col0\" class=\"data row1 col0\" >13.113996</td>\n",
       "      <td id=\"T_4f0c0_row1_col1\" class=\"data row1 col1\" >25.950253</td>\n",
       "      <td id=\"T_4f0c0_row1_col2\" class=\"data row1 col2\" >0.494930</td>\n",
       "      <td id=\"T_4f0c0_row1_col3\" class=\"data row1 col3\" >1.336286</td>\n",
       "      <td id=\"T_4f0c0_row1_col4\" class=\"data row1 col4\" >45.120367</td>\n",
       "      <td id=\"T_4f0c0_row1_col5\" class=\"data row1 col5\" >0.049569</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x28426aff6d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_results.sort_index(axis=1,level=[0,1],ascending=[True,True]).style.highlight_min(axis=0, color='lightgreen')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0cc5ee-68ea-46fb-b698-a8726395e211",
   "metadata": {},
   "source": [
    "Utilizing the model converted to .onnx one can reduce inference time on CPU by 1.5-1.7 times, whereas there is no improvement in case of computations on GPU. Moreover, high batches for .onnx model can significantly worse inference time on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21d3c7e-8f04-4530-bb0a-7b62039e7736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "product_classifier",
   "language": "python",
   "name": "product_classifier"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
